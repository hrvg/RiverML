% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_learners.R
\name{get_learners_internal}
\alias{get_learners_internal}
\title{Internal function to get the learners}
\usage{
get_learners_internal(
  lrn_ids,
  data = smote_data,
  inner_resampling = inner,
  grid_resolution = tuneLength,
  .info = FALSE,
  pca = FALSE,
  corr = FALSE,
  prob = TRUE,
  randomit = 100,
  mes
)
}
\arguments{
\item{lrn_ids}{\code{character}, list of \code{mlr} learner idenfitiers}

\item{data}{a named list with two elements \code{data} and \code{labels}}

\item{inner_resampling}{\code{resampleDesc} from \code{mlr}, the inner folds of the nested resampling}

\item{grid_resolution}{\code{numeric}, defines the granularity of the discrete tuning grid}

\item{.info}{\code{logical}, controls the amount of information printed when tuning}

\item{corr}{\code{logical}, are highly correlated predictors removed}

\item{mes}{\code{mlr} list of measure to compute while tuning, the learner are tuning against the first element}

\item{.pca}{\code{logical}, is a PCA performed}
}
\value{
a list of \code{mlr} learners
}
\description{
The following learners \verb{"classif.h2o.glm", "classif.lda", "classif.mda", "classif.naiveBayes", "classif.IBk", "classif.kknn", "classif.featureless"} do not get transformed into a \code{TuneWrapper}.
The following learners \code{c("classif.h2o.gbm", "classif.h2o.deeplearning", "classif.nnTrain", "classif.xgboost")} are tuned with \code{mlr::makeTuneControlRandom()}.
All other learners are tuned with \code{mlr::makeTuneControlGrid()}.
Setting the possible hyper-parameters values is handled by \code{get_ps()}
}
\keyword{ml-learners}
