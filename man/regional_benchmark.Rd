% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_benchmark.R
\name{regional_benchmark}
\alias{regional_benchmark}
\title{Wrapper function to compute the benchmark}
\usage{
regional_benchmark(
  regions = c("ALLSAC", "SFE", "K", "NC", "NCC", "SCC", "SC", "SJT"),
  LRN_IDS,
  TUNELENGTH,
  INNER,
  ITERS,
  PROB,
  NU,
  REPS,
  PREPROC,
  FINAL,
  PATH,
  REDUCED,
  MES,
  INFO,
  FS,
  FS_NUM
)
}
\arguments{
\item{regions}{\code{character}, vector of \code{region} identifiers (e.g. \code{"SFE"})}

\item{LRN_IDS}{\code{character}, vector of learner identifiers (e.g. \code{"classif.randomForest"})}

\item{TUNELENGTH}{\code{numeric}, controls the number of hyper-parameters for discrete tuning}

\item{INNER}{\code{ResampleDesc},  the inner folds for the nested resampling}

\item{ITERS}{\code{numeric}, controls the number of tries for the random tuning}

\item{PROB}{\code{logical}, selects the type of output, if \code{TRUE} probabilities, if \code{FALSE} response}

\item{NU}{\code{numeric}, number of folds for the \eqn{\nu}-fold cross-validation}

\item{REPS}{\code{numeric}, number of repetitions for the repeated \eqn{\nu}-fold cross-validation}

\item{PREPROC}{\code{character}, vector of preprocessing identifiers (e.g. \code{"scale"})}

\item{FINAL}{\code{logical}, selects the type of runs, if \code{FALSE} trains, else predict}

\item{PATH}{\code{character} or \code{file.path}, path to the output directory}

\item{REDUCED}{\code{logical}, legacy option}

\item{MES}{\code{list}, list of measures from the \code{mlr} package, the first one is optimized against}

\item{INFO}{\code{logical}, controls the information printed by the training process}

\item{FS}{\code{logical}, if \code{TRUE} activates the feature selection}

\item{FS_NUM}{\code{numeric}, number of features to select if \code{FS} is \code{TRUE}}
}
\value{
a list of \code{mlr} benchmark results
}
\description{
\code{regional_benchmark()} is a wrapper function calling a number of functions. See details.
}
\details{
Here is a some pseudo-code that explains what is happening behind the scenes.
\enumerate{
\item \emph{Skip}. Because \code{regional_benchmark()} is called inside the \code{for}-loop \verb{for (FS_NUM in FS_NUM_LIST)} (see above), if \code{FINAL} is #' \code{TRUE}, \code{regional_benchmark()} skips the \code{region} it does not need to calculate the final models.
\item \emph{Data loading}. This handled by \code{get_training_data()}
\item \emph{Data formatting}. This is handled by \code{fmt_labels()}, \code{sanitize_data()} and \code{get_coords()}.
\item \emph{Feature selection}. If \code{FINAL} is \code{TRUE} the selected features are retrieved from  \code{ get_bestBMR_tuning_results()}. If \code{FINAL} is \code{FALSE} the selected features are derived from transformed training data using \code{get_ppc()} and \code{preproc_data()}. The resulting transformed data are filtered for correlation higher than 0.95 with \code{caret::findCorrelation()}. Then, 500 subsampled \code{mlr} Tasks are created with \code{mlr::makeResampleDesc()}, \code{mlr::makeClassifTask()}, \code{mlr::makeResampleInstance()} and \code{mlr::filterFeatures()}. The #' \code{FS_NUM} most commonly select features across the 500 realizations are selected.
\item \emph{Pre-processing}. The target and training data are transformed using \code{get_ppc()} on the target data and \code{preproc_data()} on the #' training data. SMOTE is applied using \code{get_smote_data()} and \code{get_smote_coords()} which both call \code{resolve_class_imbalance()}.
\item \emph{Tasks}. Tasks are obtained using \code{mlr::makeClassifTask()}.
\item \emph{Learners}. Learners are constructed using \code{get_learners()} or \code{get_final_learners()}.
\item \emph{Compute benchmark}. The benchmark is run with \code{compute_final_model()} or \code{compute_benchmark()} (which needs to retrieve the outer #' folds of the nested resampling with \code{get_outers()}).
}
}
